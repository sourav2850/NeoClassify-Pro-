{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HYAhlNXw8ei0"},"source":["#CSE5DL Assignment\n","\n","### Assignment due date: Friday 26/5/2023\n","\n","Penalties are applied to late assignments (accepted up to 5 business days after the due date only). Five percent is deducted per business day late. A mark of zero will be assigned to assignments submitted more than 5 days late. \n","\n","<font color='red'> This is an individual assignment. You are not permitted to work as a part of a group when writing this assignment. </font>\n","\n","### Assignment submission\n","\n","Please zip all `*.ipynb`, `*.py`, `*.docx` and `*.xlsx` files into a single zip file and submit the zipped file via the link provided on LMS. \n","\n","### Copying, Plagiarism\n","Plagiarism is the submission of somebody else’s work in a manner that gives the impression that the work is your own. For individual assignments, plagiarism includes the case where two or more students work collaboratively on the assignment.  The Department of Computer Science and Information Technology treats plagiarism very seriously.  When it is detected, penalties are strictly imposed.\n","\n","### ChatGPT \n","A key purpose of this assessment task is to test your own ability to complete the assigned tasks.  Therefore, the use of ChatGPT, AI tools or chatbots with similar functionality is prohibited for this assessment task. Students who are found to be in breach of this rule will be subject to normal academic misconduct measures.  Additionally, students may be engaged to provide an oral validation of their understanding of their submitted work (e.g. coding).\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9yQ1TwYgWS0z"},"source":["\n","# Introduction\n","\n","**DESCRIPTION:** In this assignment we have provided you with skeleton code. We have an image dataset and a text dataset, and you must train deep learning models for them. With the exception of Task 2b, all of the code required has already been shown to you in the labs.\n","\n","In this assignment you will be required to write code and write short answer responses to questions in a structured report. You have been provided with a template Word document of this report in which you simply have to fill in the blanks (1-3 sentences is expected).\n","\n","Throughout this assignment, there are a few challenge questions worth bonus marks. Task 1 is worth 66 marks and Task 2 is worth 32 marks, totalling 98 marks possible before challenge questions. You can receive up to 10 marks from the bonus questions, so the maximum number of marks you can get is 108. However if you get over 100 marks the actual mark you will receive is 100% for the assignment assessment component of your grades.  Unless otherwise stated all marks quoted do not include challenge questions.\n","\n","There are 71 marks associated with code and 27 marks associated with the report.\n","\n","**INSTRUCTIONS:**\n","\n","1.   Copy the skeleton files to your Google Drive.\n","2.   Edit `SKELETON_DIR` in the first cell to point to the skeleton files you uploaded in step 1. The provided code assumes you have uploaded them to \"Uni/CSE5DL/Assignment\" in your Google Drive.\n","3.   Run the following two cells\n"]},{"cell_type":"code","metadata":{"id":"DI_h0wYR8WwZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679957312575,"user_tz":-660,"elapsed":102297,"user":{"displayName":"Zhen He","userId":"07388169493391803483"}},"outputId":"5891a459-d512-49b4-948e-60a9f75987e5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set the working directory for the assignment\n","import os\n","#SKELETON_DIR = '/content/drive/MyDrive/Uni/CSE5DL/Assignment'\n","SKELETON_DIR = '/content/drive/MyDrive/CSE5DL 2023/Assignment/Assignment 2023 release'\n","\n","os.chdir(SKELETON_DIR)\n","! mkdir -p \"$SKELETON_DIR/saved_models\"\n","! mkdir -p \"$SKELETON_DIR/logs\"\n","\n","# Set up auto-reloading modules from the working directory\n","%load_ext autoreload\n","%autoreload 2\n","\n","# Install extra dependencies\n","!pip install -q transformers==4.27.0\n","!pip install -q wandb==0.15.0\n","!pip install -q torchmetrics==0.11.3\n","\n","\n","# Set the default figure size\n","import matplotlib.pyplot as plt\n","plt.rcParams['figure.dpi'] = 120"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/97.4 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 KB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.6/518.6 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","metadata":{"id":"McyD2WEJJDz6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679957397158,"user_tz":-660,"elapsed":23615,"user":{"displayName":"Zhen He","userId":"07388169493391803483"}},"outputId":"9e668eb4-b23d-4aee-babd-4d2bc3acd1ab"},"source":["%%shell\n","DATA_URL='1Z8Sg1a3T1HTJDmsM5Q_SaVslKkH3U3An'\n","\n","pip install --upgrade --no-cache-dir gdown\n","pushd /content\n","gdown  $DATA_URL\n","unzip -q data.zip\n","popd"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gdown in /usr/local/lib/python3.9/dist-packages (4.6.4)\n","Collecting gdown\n","  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown) (4.65.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown) (1.16.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown) (3.10.2)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.9/dist-packages (from gdown) (2.27.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown) (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown) (2.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (2022.12.7)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (1.7.1)\n","Installing collected packages: gdown\n","  Attempting uninstall: gdown\n","    Found existing installation: gdown 4.6.4\n","    Uninstalling gdown-4.6.4:\n","      Successfully uninstalled gdown-4.6.4\n","Successfully installed gdown-4.7.1\n","/content /content/drive/MyDrive/CSE5DL 2023/Assignment/Assignment 2023 release\n","Downloading...\n","From (uriginal): https://drive.google.com/uc?id=1Z8Sg1a3T1HTJDmsM5Q_SaVslKkH3U3An\n","From (redirected): https://drive.google.com/uc?id=1Z8Sg1a3T1HTJDmsM5Q_SaVslKkH3U3An&confirm=t&uuid=3b11db2e-5da4-4f16-a009-b80bbc9f0ec7\n","To: /content/data.zip\n","100% 1.04G/1.04G [00:07<00:00, 136MB/s]\n","/content/drive/MyDrive/CSE5DL 2023/Assignment/Assignment 2023 release\n"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"0UMZDfCl9uzf"},"source":["# Task 1 - Image Classification\n","\n","**MARKS**: 66\n","\n","In this first task, you will create a deep learning model to classify images of skin lesions into one of seven classes: \n","\n","1.   \"MEL\" = Melanoma\n","2.   \"NV\" = Melanocytic nevus\n","3.   \"BCC\" = Basal cell carcinoma\n","4.   \"AKIEC\" = Actinic keratosis\n","5.   \"BKL\" = Benign keratosis\n","6.   \"DF\" = Dermatofibroma\n","7.   \"VASC\" = Vascular lesion\n","\n","The data for this task is a subset of: https://challenge2018.isic-archive.com/task3/\n","\n","The data for this task is inside the `/content/data/img` folder. It contains ~3,800 images named like `ISIC_000000.jpg` and the following label files:\n","\n","*   `/content/data/img/train.csv`\n","*   `/content/data/img/val.csv`\n","*   `/content/data/img/train_small.csv`\n","*   `/content/data/img/val_small.csv`\n","\n","The `small` versions are the first 200 lines of each partition and are included for debugging purposes. To save time, ensure your code runs on the `small` versions first.\n","\n","**NOTE**: To explore the labels, you can click the above hyperlinks to open the relevant csv file."]},{"cell_type":"markdown","metadata":{"id":"VXe_oJsh2v0R"},"source":["## Task 1a. Explore the training set\n","\n","**MARKS**: 5 (Code 3, Reports 2)\n","\n","**INSTRUCTIONS**: Check for data issues, as we have done in the labs. Check the class distribution and at least 1 other potential data issue. Hint: Look in `explore.py` for a function that can plot the class distribution.\n","\n","**REPORT**: What did you check for? What data issues are present in this dataset?"]},{"cell_type":"code","metadata":{"id":"tvZcHWR_nrN_","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1679957446443,"user_tz":-660,"elapsed":622,"user":{"displayName":"Zhen He","userId":"07388169493391803483"}},"outputId":"08808d33-b400-493d-b257-beb5f0028bbe"},"source":["import pandas as pd\n","\n","IMG_CLASS_NAMES = [\"MEL\", \"NV\", \"BCC\", \"AKIEC\", \"BKL\", \"DF\", \"VASC\"]\n","\n","train_df = pd.read_csv('/content/data/img/train.csv')\n","val_df = pd.read_csv('/content/data/img/val.csv')\n","train_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          image  MEL   NV  BCC  AKIEC  BKL   DF  VASC\n","0  ISIC_0024306  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n","1  ISIC_0024307  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n","2  ISIC_0024308  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n","3  ISIC_0024309  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n","4  ISIC_0024310  1.0  0.0  0.0    0.0  0.0  0.0   0.0"],"text/html":["\n","  <div id=\"df-e55851fb-0237-4d30-b8a8-0b79fdda466e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>MEL</th>\n","      <th>NV</th>\n","      <th>BCC</th>\n","      <th>AKIEC</th>\n","      <th>BKL</th>\n","      <th>DF</th>\n","      <th>VASC</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ISIC_0024306</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ISIC_0024307</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ISIC_0024308</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ISIC_0024309</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ISIC_0024310</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e55851fb-0237-4d30-b8a8-0b79fdda466e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e55851fb-0237-4d30-b8a8-0b79fdda466e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e55851fb-0237-4d30-b8a8-0b79fdda466e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"dpqxCqPth8va"},"source":["from PIL import Image\n","# Change the filename to view other examples from the dataset \n","display(Image.open('/content/data/img/ISIC_0024306.jpg'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J6nJ9TKkDM_E"},"source":["import explore\n","\n","# TODO - Check for data issues\n","# Hint: You can convert from one-hot to integers with argmax\n","#       This way you can convert 1, 0, 0, 0, 0, 0, 0  to class 0 \n","#                                0, 1, 0, 0, 0, 0, 0  to class 1\n","#                                0, 0, 1, 0, 0, 0, 0  to class 2\n","# so it should be something like the following: \n","# train_labels = train_df.values[....].argmax(....)\n","# val_labels = val_df.values[....].argmax(....)\n","#     - you need to fill in the ... parts with the correct values.\n","# You should then print output the contents of train_labels to see if \n","# it matches the contents of train.csv\n","#\n","# Next you can plot the class distributions like the following:\n","# explore.plot_label_distribution(....)\n","#    - do the above for both the train and val labels.\n","#\n","# Following this look for other potential problems with the data\n","#   You can look at lab 2a to see what was checked there.\n","#   You may also think of any other potential problems with the data.\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EzxriiNQ22CG"},"source":["## Task 1b. Implement Training loop\n","\n","**MARKS**: 17 (Code 15, Reports 2)\n","\n","**INSTRUCTIONS**:\n","\n","*   Implement LesionDataset in `datasets.py`. Use the cell below to test your implementation. \n","*   Implement the incomplete functions in `train.py` marked as \"Task 1b\"\n","*   Go to the [Model Training Cell](#task-1-model-training) at the end of Task 1 and fill in the required code for \"Task 1b\".\n","\n","**REPORT**: Why should you *not use* `random_split` in your code here?"]},{"cell_type":"code","metadata":{"id":"-uZTyqK9XvsJ"},"source":["import datasets\n","\n","ds = datasets.LesionDataset('/content/data/img',\n","                            '/content/data/img/train.csv')\n","input, label = ds[0]\n","print(input)\n","print(label)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dg_P1_Pd26Bm"},"source":["## Task 1c. Implement a baseline convolutional neural network\n","\n","**MARKS**: 17 (Code 12, Reports 5)\n","\n","You will implement a baseline convolutional neural network which you can compare results to. This allows you to evaluate any improvements made by hyperparameter tuning or transfer learning.\n","\n","**INSTRUCTIONS**:\n","\n","*   Implement a `SimpleBNConv` in `models.py` with:\n","    *   5 `nn.Conv2d` layers, with 8, 16, 32, 64, 128 output channels respectively, with the following between each convolution layer:\n","        *   `nn.ReLU()` for the activation function, and\n","        *   `nn.BatchNorm2d`, and\n","        *   finally a `nn.MaxPool2d` to downsample by a factor of 2.\n","*   Use a normalised confusion matrix on the model's validation predictions in `train.py`.\n","*  Go to the [Model Training Cell](#task-1-model-training) at the end of Task 1 and fill in the required code to train the model.\n","\n","Training should take about 1 minute/epoch. Validation accuracy should be 60-70%, but UAR should be around 20-40%.\n","\n","**REPORT**: As training sets get larger, the length of time per epoch also gets larger. Some datasets take over an hour per epoch. This makes it impractical to debug typos in your code since it can take hours after starting for the program to reach new code. Name two ways to significantly reduce how long each epoch takes - for debugging purposes - while still using real data and using the real training code.\n","\n","**REPORT**: Show the confusion matrix and plots of the validation accuracy and UAR in your report, and explain what is going wrong. \n","(Right-click a plot and select \"save image as...\" to save the image to your computer)"]},{"cell_type":"markdown","metadata":{"id":"bnlbHlO953Dw"},"source":["## Task 1d. Account for data issues\n","\n","> Indented block\n","\n","\n","\n","**MARKS**: 6 (Code 3, Reports 3)\n","\n","**INSTRUCTIONS**: Account for the data issues in Task 1a and retrain your model.\n","\n","**REPORT**: How did you account for the data issues? Was it effective? How can you tell? Show another confusion matrix.\n","\n","**IMPORTANT NOTE**: One of the techniques from the lab will cause a warning in the metric calculation on `train_small.csv`, but will work fine on `train.csv`."]},{"cell_type":"markdown","metadata":{"id":"z4bd8vMQ3C6b"},"source":["## Task 1e. Data Augmentation\n","\n","**MARKS**: 6 (Code 3, Reports 3)\n","\n","**INSTRUCTIONS**: \n","\n","*   Add an `augment` flag to LesionDataset which specifies whether any augmentation is done to the images. Ensure it is set to `True` *only* for the training dataset.\n","*   Use random horizontal flips\n","*   Use at least 2 other different non-deterministic augmentations\n","\n","**REPORT:** Are random vertical flips appropriate for this dataset? Why?\n","\n","Using data augmentation does not guarantee improved model performance. Data augmentation can hurt test performance by making the model train on unrealistic images.\n","\n","**REPORT**: What effect did Data Augmentation have on performance? Show a screenshot of the relevant graphs from Weights & Biases for evidence.\n","\n","**CHALLENGE**: (3 marks) Apply 5 crop augmentation with crop size 200x300. Make a distinct model which uses 5 crops at once to give a single answer. Include in your report how you did this and report the effect on performance."]},{"cell_type":"markdown","metadata":{"id":"3DkP5Mg48Gm1"},"source":["## Task 1f. Chase improved performance\n","\n","**MARKS**: 15 (Code and reports not separable for this task)\n","\n","**INSTRUCTIONS**: \n","*   Create a model from a pre-trained model from the torchvision model zoo. We recommend Resnet18, but you may use any model you like. You may freeze the weights of all layers except the last, or fine-tune all the weights. https://cloudstor.aarnet.edu.au/plus/s/TsYJXyJWch0h7TD\n","*   Create your own models, modifying the model architecture, try different losses, learning rates. Change anything you like except the evaluation metrics in search of a better model.\n","\n","Train at least 10 different models, each with a different combination.\n","\n","**REPORT**: Create a table in an excel spreadsheet that is similar to that used in Lab 3 to record your results. Make sure it includes every parameter of variation between your combinations as a separate column. Include notes about what you were thinking/hoping for each combination as a number column in the spreadsheet.\n","\n","In addition to the excel spreadsheet generate a report using Weights and Biases of the models you trained and the performance curves. Save the report as a pdf and include this in your submission. Please see this link on how to generate reports with Weights and Biases. https://docs.wandb.ai/guides/reports\n","\n","Play around with Weights and Biases to see what cool features you can dig out and use to better visualize the training results and use that to improve the information shared via the report. \n","\n","Write a discussion about the key findings from the experimental results.\n","\n","**CHALLENGE REPORT**: (3 marks) Assuming you use the full dataset in a single epoch, if you halve the size of the batch size, what happens to the number of times that you update the weights per epoch? With reference to the gradients, under what circumstances is this good?"]},{"cell_type":"markdown","metadata":{"id":"RxhIzDQjt4mu"},"source":["<a name=\"task-1-model-training\"></a>\n","## Model Training Cell\n","\n","Note we will be using Weights and Biases to keep track of our experimental runs and evaluation metrics. This is similar to lab 6. Please see lab 6 to learn how to use Weights and Biases. \n"]},{"cell_type":"code","metadata":{"id":"2tFG3cT2i53Q"},"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torch.utils.data.sampler import WeightedRandomSampler\n","\n","import datasets\n","import models\n","import train\n","\n","torch.manual_seed(42)\n","\n","NUM_EPOCHS = 5\n","BATCH_SIZE = 64\n","\n","# Create datasets/loaders\n","# TODO Task 1b - Create the data loaders from LesionDatasets\n","# TODO Task 1d - Account for data issues, if applicable\n","# train_dataset = ...\n","# val_dataset = ...\n","# train_loader = ...\n","# val_loader = ...\n","\n","\n","# Instantiate model, optimizer and criterion\n","# TODO Task 1c - Make an instance of your model, specifiy the optimizer and criterion you want to use\n","# TODO Task 1d - Account for data issues, if applicable\n","# model = ...\n","# optimizer = ...\n","# criterion = ...\n","\n","# Train model\n","# TODO Task 1c: Set ident_str to a string that identifies this particular\n","#               training run. Note this line in the training code\n","#                     exp_name = f\"{model.__class__.__name__}_{ident_str}\"\n","#               So it means the the model class name is already included in the\n","#               exp_name string. You can consider adding other information \n","#               particular to this training run, e.g. learning rate (lr) used, \n","#               augmentation (aug) used or not, etc.\n","\n","train.train_model(model, train_loader, val_loader, optimizer, criterion,\n","                  IMG_CLASS_NAMES, NUM_EPOCHS, project_name=\"CSE5DL Assignment Task 1\",\n","                  ident_str= \"fill me in here\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nHmk8S-B3PkF"},"source":["# Task 2 - News article classification\n","\n","**MARKS**: 32\n","\n","You will first create your own model to classify news articles into one of the following classes:\n","\n","*   World\n","*   Sport\n","*   Business\n","*   Sci/Tech\n","\n","You will then compare it to a pre-trained DistilBERT model that has been fine-tuned, similar to Lab 6. Note: using a model pre-trained on a source task for a new target task is called \"transfer learning\" whether you fine-tune it or not.\n","\n","The data for this task is a subset of: https://github.com/mhjabreel/CharCnn_Keras/tree/master/data/ag_news_csv"]},{"cell_type":"markdown","metadata":{"id":"DThd7TE73SVK"},"source":["## Task 2a. Exploring the dataset\n","\n","**MARKS**: 3 (Code 2, Reports 1)\n","\n","**INSTRUCTIONS**: Check for at least 2 data issues.\n","\n","**REPORT**: What did you check for? What data issues exist, if any? Report anything you checked even if it turned out the data did not have that issue. We want to know what you are checking."]},{"cell_type":"code","metadata":{"id":"jJdQupWVOuOw","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1679958013753,"user_tz":-660,"elapsed":313,"user":{"displayName":"Zhen He","userId":"07388169493391803483"}},"outputId":"8c457aba-30d0-445d-a1d0-e72e358ffec8"},"source":["import pandas as pd\n","\n","with open('/content/data/txt/classes.txt') as f:\n","    TXT_CLASS_NAMES = [line.rstrip('\\n') for line in f]\n","\n","train_df = pd.read_csv('/content/data/txt/train.csv', header=None)\n","val_df = pd.read_csv('/content/data/txt/val.csv', header=None)\n","train_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   0                                                  1  \\\n","0  3  Wall St. Bears Claw Back Into the Black (Reuters)   \n","1  3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n","2  3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n","3  3  Iraq Halts Oil Exports from Main Southern Pipe...   \n","4  3  Oil prices soar to all-time record, posing new...   \n","\n","                                                   2  \n","0  Reuters - Short-sellers, Wall Street's dwindli...  \n","1  Reuters - Private investment firm Carlyle Grou...  \n","2  Reuters - Soaring crude prices plus worries\\ab...  \n","3  Reuters - Authorities have halted oil export\\f...  \n","4  AFP - Tearaway world oil prices, toppling reco...  "],"text/html":["\n","  <div id=\"df-2c2d31b6-bf5d-450b-bf0c-89950da5abc2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3</td>\n","      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n","      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3</td>\n","      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n","      <td>Reuters - Private investment firm Carlyle Grou...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n","      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n","      <td>Reuters - Authorities have halted oil export\\f...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3</td>\n","      <td>Oil prices soar to all-time record, posing new...</td>\n","      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c2d31b6-bf5d-450b-bf0c-89950da5abc2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2c2d31b6-bf5d-450b-bf0c-89950da5abc2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2c2d31b6-bf5d-450b-bf0c-89950da5abc2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"8ULAXaX9F1SJ"},"source":["import explore\n","# TODO Check for data issues.\n","# Again you should fill in the following:\n","# train_labels = ...\n","# val_labels = ....\n","#   - Note the csv file has class labels start from 1 but\n","#     pytorch expects class labels to start from 0 instead. \n","#\n","# explore.plot_label_distribution(....) for train labels\n","# explore.plot_label_distribution(....) for val labels\n","# \n","# check for other kinds of problems with the data like you did for Task 1a.\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AiR4YMsC3WTf"},"source":["## Task 2b. Clustering and visualising embeddings from a pre-trained model\n","\n","**MARKS**: 14 (Code 11, Reports 3)\n","\n","**INSTRUCTIONS**: \n","\n","*  Implement the `TextDataset` class in the `datasets.py` file. Consider adding a small code block to test your implementation, as provided in task 1b.\n","\n","*   Complete `visualise_embeddings.py` and run it. Make sure you instantiate two different models to visualize. One is the sequence classification model and the other is the token classification model. For the sequence classification model the code will visualize the CLS token. For the token classification model the model will perform average pool over all output tokens except the CLS token output. If you don't understand this. Please take a look at the Lab 6 demo notebook.\n","* The `visualise_embeddings.py` file does the following:\n","    *   visualise embeddings of the news articles from the two pre-trained `'distilbert-base-uncased'` model (i.e. the models which have not yet been fine-tuned on the labels) using T-SNE. T-SNE is a popular dimensionality reduction method that takes data from a high dimensional space and reduces it to just two dimensions while trying to preserve the right distances between points. The visualization will represent each article by a point with a color corresponding to their true label. Ideally the colors are well separated into separate clusters. If this happens it will be really cool since it means we did not even need to fine-tune the model on our data, it is already able to separate the classes.\n","    *   Next the code will run K-Means clustering on the validation set to group the data into separate clusters. The code will then colour the points based on which cluster they belong to rather than the ground truth label. \n","\n","\n","**REPORT**: By looking at the resulting images of the two models (sequence classification and token classification), which two classes have the most similar embeddings? How can you tell? Did you expect this, if so, why, if not why not?\n","\n","**CHALLENGE**: (8 marks) Only attempt this after completing the rest of Task 2.\n","\n","*   Modify `visualise_embeddings.py` so that it can load the weights for a fine-tuned DistilBERT model. Then visualize the data points with their corresponding true labels. \n","*   Next instead of using K-Means for the second visualisation, use the model's own predicted labels to colour the points.\n","\n","Present the resulting images in your report."]},{"cell_type":"code","metadata":{"id":"f2yOEBQcF2yO"},"source":["import visualise_embeddings\n","SENTENCE_LEN = 80\n","# Run this code to visualize the results from embedding text using the sequence classification model\n","visualise_embeddings.mk_plots(SENTENCE_LEN, sequenceClassificationModel = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import visualise_embeddings\n","SENTENCE_LEN = 80\n","# Run this code to visualize the results from embedding text using the token classification model\n","visualise_embeddings.mk_plots(SENTENCE_LEN, sequenceClassificationModel = False)"],"metadata":{"id":"zfw82WIyWckq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aGx4lFbx3bS5"},"source":["## Task 2c. Models\n","\n","**MARKS**: 10 (Code 7, Reports 3)\n","\n","**INSTRUCTIONS**:\n","\n","*   Complete `TextMLP` in `models.py`. It should be a simple MLP with 8 Linear layers. It should first embed the inputs into a vocabulary of size 30522. Use an output feature size of 256 in all hidden layers and a feature size of 128 for the embeddings. Flatten the sentence after embedding, but before it goes into any Linear layers. Use batch norm and ReLU. Train for 1000 epochs with learning rate of 0.001 and a batch size of 512.\n","*   Complete `DistilBertForClassification` in `models.py`. This model should replace the last layer with an `nn.Linear` with 4 outputs for classification. Hint: Call `print()` on the DistilBERT model to observe the layers and their names before attempting this. Train for 4 epochs with learning rate of 0.001 and a batch size of 64.\n","\n","Each of these should take around 10 minutes to complete.\n","\n","Go to the [Model Training Cell](#task-2-model-training) at the end of Task 2 and fill in the required code to train the model.\n","\n","**REPORT**: The saved model weights of a fine-tuned DistilBERT model are >200MB, but you only created one small `nn.Linear` layer. Why is the saved model so large? \n","\n","**REPORT**: These models should accept only input with a dtype of `torch.int64`. What do each of these longs (`int64`) represent?"]},{"cell_type":"markdown","metadata":{"id":"IRhQjiDtQbFM"},"source":["## Task 2d. Learning Rate\n","\n","**MARKS**: 5 (Code 0, Reports 5)\n","\n","Fine-tuning `DistilBertForSequenceClassification` with Adam at a learning rate of 0.001 results in very poor accuracy (~26%).\n","\n","**INSTRUCTIONS**: \n","\n","*   Uncomment the lines marked `Task 2d` in `train.py`\n","*   Execute the below cell to begin training and observe the class distribution per batch\n","*   Comment the lines marked `Task 2d` in `train.py` so they no longer interfere with the training.\n","\n","\n","**REPORT**: What is wrong with the class distributions? The learning rate can be changed to fix it. Should you increase or decrease the learning rate? How can you tell?\n","\n","**REPORT**: After fixing the learning rate, comment on the relative train/val performance between these two models. Which model performed better on each partition? Is this expected? If so, why?\n","\n","When you have finished Task 2d. Go back to Task 2b and finish the challenge if you are up to it. You should get a pleasant surprise if you have done everything correctly.\n"]},{"cell_type":"markdown","metadata":{"id":"jkIB_B2t_yps"},"source":["<a name=\"task-2-model-training\"></a>\n","## Model Training Cell"]},{"cell_type":"code","metadata":{"id":"Sb3PAZIGF35d"},"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","\n","import datasets\n","import models\n","import train\n","\n","torch.manual_seed(42)\n","\n","SENTENCE_LEN = 80\n","NUM_EPOCHS = 4\n","BATCH_SIZE = 64\n","\n","# Create datasets/loaders\n","# TODO: Create the data loaders from TextDatasets\n","# train_dataset = ...\n","# val_dataset = ...\n","# train_loader = ...\n","# val_loader = ...\n","\n","\n","# Instantiate model, optimizer and criterion\n","# TODO: Make an instance of your model\n","# model = models.<**put the name of the model class you created in the model file here**>\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","criterion = nn.CrossEntropyLoss()\n","\n","# TODO Change ident_str to something that identifying this experiment e.g. lr0001\n","# Train model. We are using the same train model function we wrote for task 1.\n","train.train_model(model, train_loader, val_loader, optimizer, criterion,\n","                  TXT_CLASS_NAMES, NUM_EPOCHS, project_name = \"CSE5DL Assignment Task 2\",\n","                  ident_str='**fill me in**')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bo9kWUc1ga2_"},"source":["# Super challenge task\n","\n","This challenge task is quite difficult and will really test your mastery of PyTorch and `nn.Linear` layers.\n","\n","**MARKS**: 5\n","\n","We can manually assign weights to an `nn.Linear` like this:\n"]},{"cell_type":"code","metadata":{"id":"WNSRGuIFhPD8"},"source":["import torch\n","import torch.nn as nn\n","lin = nn.Linear(10, 20)\n","manual_weights = torch.arange(20*10).reshape(lin.weight.shape)\n","lin.weight.data[:] = manual_weights\n","lin.bias.data[:] = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IY8ecPzVhqeL"},"source":["But this does not calculate anything useful. A Linear layer simply performs a weighted sum (plus bias). We can choose weights/biases to perform known operations.\n","\n","**INSTRUCTIONS**: \n","1.   Given an `nn.Linear(1, 1)` layer, set the weights such that the layer adds 1 to it's input.\n","2.   Given an `nn.Linear(1, 1)` layer, set the weights such that the layer calculates `y = 3x + 2`.\n","3.   Given an `nn.Linear(4, 1)` layer, set the weights such that the layer calculates the average of it's inputs.\n","4.   Given an `nn.Linear(4, 2)` layer, set the weights such that the layer calculates both the average of it's inputs and the sum of the inputs.\n","5.   Given an `nn.Linear(3, 3)` layer, set the weights such that the layer returns the inputs, but in reverse order.\n","6.   Given an `nn.Linear(5, 2)` layer, set the weights such that the layer always returns `(4,2)`\n","\n","\n","Note: We would never use this in a deep learning model; this challenge is to prove that you understand the mathematics and coding mechanics of the `nn.Linear` layer."]},{"cell_type":"code","metadata":{"id":"py6eOCV4hp8z"},"source":["import sc1\n","sc1.test_1(sc1.modify_lin_1)\n","sc1.test_2(sc1.modify_lin_2)\n","sc1.test_3(sc1.modify_lin_3)\n","sc1.test_4(sc1.modify_lin_4)\n","sc1.test_5(sc1.modify_lin_5)\n","sc1.test_6(sc1.modify_lin_6)"],"execution_count":null,"outputs":[]}]}